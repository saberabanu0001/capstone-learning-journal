# üìÖ Day-4: Spatial Object Detection (What + Where)

## üîó Recap
- **Day-2**: The robot can see **RGB frames** and **depth maps**.  
- **Day-3**: The robot can use a **Neural Network** to detect objects (like ‚Äúperson‚Äù, ‚Äúdog‚Äù, ‚Äúcar‚Äù).  
- **Day-4**: Combine them ‚Üí not only *‚Äúwhat object is this?‚Äù* but also *‚Äúhow far away is it?‚Äù*  

---

## ‚ö° Why This Matters
For the project:
- **Week-6 = Person Detection** ‚úÖ  
- But the robot doesn‚Äôt just need to know *‚Äúa person exists.‚Äù*  
- It must also know *‚Äúwhere they are‚Äù* (distance + position).  

This allows the robot to:
- Avoid obstacles.  
- Follow people.  
- Interact with the environment intelligently.  

---

## ü§ñ What `spatial_mobilenet.py` Does
Think of it like a recipe your OAK-D Lite camera follows:

### 1. Load a brain (the AI model)
- Loads a **MobileNetSSD model**.  
- Pre-trained to recognize ~20 everyday objects (person, car, dog, chair, etc.).  

### 2. Turn on the cameras
- **RGB camera**: captures color images for object detection.  
- **Two mono cameras**: capture left + right grayscale views for depth.  
- **StereoDepth node**: calculates how far things are (like human eyes).  

### 3. Run detection with depth awareness
- RGB frames go to the **Myriad X chip** running MobileNetSSD.  
- StereoDepth provides a **depth map (in millimeters)**.  
- The **Spatial Detection Network** fuses both ‚Üí each detection has a label + 3D coordinates.  

### 4. Send results back to computer/Jetson
- The program outputs:  
  - Live RGB video feed.  
  - List of detections (object name, confidence, bounding box).  
  - **Spatial coordinates (X, Y, Z in mm)** for each object.  

### 5. Draw boxes + depth info on video
On your screen you see two windows:  

- **Preview (RGB feed)**:  
  - Rectangles around detected objects.  
  - Labels (e.g., ‚Äúperson‚Äù).  
  - Confidence (e.g., 91%).  
  - Coordinates: X, Y, Z in millimeters.  

- **Depth (colorized map)**:  
  - Shows distances as colors (red = near, blue = far).  
  - Bounding boxes overlaid to match detections.  

### 6. Repeat continuously
- Runs many times per second.  
- Gives **real-time 3D object detection**, e.g.:  
---

# Enhanced-----
## üìå Why you still see YOLO detections without a camera

**Right now, in our VisionSystem (modules/vision.py), we set:**

**vision = VisionSystem(simulate=True)**


#### That simulate=True flag tells the code:
üëâ ‚ÄúPretend we have a camera, and generate fake detections + fake depth values.‚Äù

#### This was done so that:

- You can develop and test the pipeline even without hardware.

- You can show progress to professor (detections, depth, etc.) before the OAK-D actually arrives.

### üìä Where the values come from

- label: random choices like "person", "car", "dog".

- confidence: random % values like 62.0%, 90.0% ‚Ä¶

- coords_mm: simulated 3D coordinates (X, Y, Z in millimeters).

**So right now ‚Üí it‚Äôs just mock data (like a fake sensor).**
**Once the OAK-D camera arrives, you‚Äôll change:**

- vision = VisionSystem(simulate=False)


## Then:

- The detections will come from the real YOLO model running on the OAK-D.

- The depth values will be measured by stereo depth sensors inside the camera.

# Output:
- [YOLO] dog (91.0%) at X=70mm, Y=63mm, Z=2184mm 
- [YOLO] car (87.0%) at X=-82mm, Y=93mm, Z=2723mm 
- [YOLO] dog (90.0%) at X=16mm, Y=-42mm, Z=1100mm 
- [YOLO] car (68.0%) at X=-69mm, Y=-28mm, Z=1107mm 
- [YOLO] car (85.0%) at X=-99mm, Y=-88mm, Z=1544mm 
- [YOLO] person (86.0%) at X=-68mm, Y=26mm, Z=817mm 
- [YOLO] person (89.0%) at X=47mm, Y=69mm, Z=1553mm 
- [YOLO] car (82.0%) at X=88mm, Y=87mm, Z=2726mm 
- [YOLO] car (92.0%) at X=-41mm, Y=-1mm, Z=1664mm 
- [YOLO] dog (76.0%) at X=-26mm, Y=9mm, Z^C, Z=2833mm 
- Traceback (most recent call last): File "/Users/saberabanu/All Drives/Personal/CapstoneRoboticsProject/