# Rovy AI Smart Robot - Complete Project Overview

## ğŸ§  System Overview

**Jetson Nano = The Powerful Brain** ğŸ§ 
- Runs all Python code, AI models (LLaVA), vision processing, and decision-making
- Handles stereo depth analysis, speech recognition, and navigation logic
- GPU-accelerated AI inference

**ESP32 = Motor Controller Base**
- Simple microcontroller that receives motor commands
- Handles low-level motor control, sensors, and hardware
- Just executes commands from Jetson Nano

---

## ğŸ¤– What is Rovy?

Rovy is an **autonomous AI-powered robot** that combines:
- **Brain**: Jetson Nano (powerful AI computer) - runs everything
- **Vision**: Oak-D stereo depth camera for 3D perception
- **AI Understanding**: LLaVA vision-language model for scene understanding
- **Voice Control**: ReSpeaker microphone array + speech recognition
- **Movement**: ESP32-based rover base (motor controller)
- **Smart Assistant**: Natural language interaction with voice commands

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    JETSON NANO - THE BRAIN ğŸ§                     â”‚
â”‚              (Powerful AI Computer - Runs Everything)            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              PYTHON SOFTWARE STACK                        â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   VISION     â”‚  â”‚     AI       â”‚  â”‚   VOICE      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   SYSTEM     â”‚  â”‚  ASSISTANT   â”‚  â”‚  ASSISTANT   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Oak-D      â”‚  â”‚ â€¢ LLaVA      â”‚  â”‚ â€¢ ReSpeaker  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   Camera     â”‚  â”‚   Model      â”‚  â”‚   Mic Array  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Depth      â”‚  â”‚ â€¢ Vision-    â”‚  â”‚ â€¢ Speech     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   Analysis   â”‚  â”‚   Language   â”‚  â”‚   Recognitionâ”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ 3D         â”‚  â”‚ â€¢ Scene      â”‚  â”‚ â€¢ TTS        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   Perception â”‚  â”‚   Understandingâ”‚ â”‚             â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ USB Serial
                              â”‚ (Motor Commands)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ESP32 ROVER BASE - Motor Controller                 â”‚
â”‚              (Low-level Hardware - Just Executes Commands)       â”‚
â”‚                                                                  â”‚
â”‚  â€¢ Motors (Left/Right wheels)                                   â”‚
â”‚  â€¢ OLED Display                                                 â”‚
â”‚  â€¢ IMU Sensor (orientation)                                     â”‚
â”‚  â€¢ Battery Monitor                                              â”‚
â”‚  â€¢ Camera Gimbal (pan/tilt)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure & File Connections

### **Core Components:**

#### 1. **`rover_controller.py`** - Hardware Interface
- **Purpose**: Communication bridge between Jetson Nano (brain) and ESP32 rover base
- **What it does**:
  - Sends motor commands from Jetson â†’ ESP32 via serial (JSON format: `{"T":1,"L":0.5,"R":0.5}`)
  - Reads sensor feedback from ESP32 â†’ Jetson (battery, IMU, wheel speeds)
  - Controls OLED display on rover
  - Controls camera gimbal (pan/tilt) on rover
- **Runs on**: Jetson Nano (the brain)
- **Communicates with**: ESP32 (motor controller)
- **Used by**: All navigation and assistant scripts

#### 2. **`oakd_depth_navigator.py`** - Vision System (YOUR AREA!)
- **Purpose**: 3D depth perception and obstacle avoidance
- **Components**:
  - `OakDDepthCamera`: Captures RGB + depth frames from Oak-D camera
  - `DepthNavigator`: Analyzes depth maps to find safe paths
- **Key Features**:
  - Stereo depth at 30 FPS
  - Divides view into 5 regions (far_left, left, center, right, far_right)
  - Calculates clearance scores for each region
  - Returns navigation commands: `forward`, `left`, `right`, `stop`
- **Used by**: `depth_llava_nav.py` (autonomous navigation)

#### 3. **`llava_cpp_navigator.py`** - AI Scene Understanding
- **Purpose**: High-level scene understanding using LLaVA vision-language model
- **What it does**:
  - Takes RGB images
  - Uses LLaVA to understand the scene
  - Provides strategic navigation guidance
- **Used by**: `depth_llava_nav.py` (combines with depth for smart navigation)

#### 4. **`depth_llava_nav.py`** - Autonomous Navigation System
- **Purpose**: Combines depth perception + AI for autonomous movement
- **How it works**:
  ```
  1. Camera Thread: Captures frames at 30 FPS â†’ puts in queue
  2. LLaVA Thread: Analyzes scenes every 15s â†’ provides strategic guidance
  3. Depth Thread: Real-time obstacle avoidance at 20 FPS â†’ executes movement
  ```
- **Decision Logic**:
  - Depth system handles immediate obstacles (safety-first)
  - LLaVA provides high-level guidance (e.g., "go toward door")
  - Combines both: follows LLaVA if depth confirms it's safe

#### 5. **`smart_assistant.py`** - Voice-Controlled Assistant
- **Purpose**: Natural language interaction with the robot
- **Components**:
  - `ReSpeakerInterface`: Voice input (microphone array)
  - `LLaVaAssistant`: AI brain (text + vision questions)
  - `TextToSpeech`: Voice output (Piper TTS or espeak)
  - `SmartAssistant`: Main orchestrator
- **Features**:
  - Wake word detection ("hey rovy", "hey rover")
  - Voice commands for movement ("go forward", "turn left")
  - Vision questions ("what do you see?")
  - Can start/stop autonomous navigation
- **Uses**: `rover_controller.py` for movement, camera for vision

---

## ğŸ”„ Data Flow Diagrams

### **Autonomous Navigation Flow** (`depth_llava_nav.py`):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AUTONOMOUS NAVIGATION                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Thread 1: Camera Capture (30 FPS)
    â”‚
    â”œâ”€> Oak-D Camera captures RGB + Depth
    â”‚
    â””â”€> Frame Queue (max 2 frames)
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                 â”‚                 â”‚
            â–¼                 â–¼                 â–¼
    Thread 2: LLaVA        Thread 3: Depth Navigation
    (Every 15s)            (20 FPS - Real-time)
            â”‚                 â”‚
            â”‚                 â”œâ”€> Analyze depth map
            â”‚                 â”‚   â€¢ 5 regions (left/center/right)
            â”‚                 â”‚   â€¢ Calculate clearance scores
            â”‚                 â”‚   â€¢ Safety-first decision
            â”‚                 â”‚
            â”œâ”€> Scene        â”‚
            â”‚   Analysis     â”‚
            â”‚   â€¢ "What do   â”‚
            â”‚     I see?"    â”‚
            â”‚   â€¢ Strategic  â”‚
            â”‚     guidance   â”‚
            â”‚                 â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            Combine Guidance
            â€¢ Depth = immediate safety
            â€¢ LLaVA = strategic direction
                    â”‚
                    â–¼
            Motor Commands
            â€¢ forward/left/right/stop
            â€¢ Speed based on clearance
                    â”‚
                    â–¼
            ESP32 Rover
            â€¢ Executes movement
```

### **Voice Assistant Flow** (`smart_assistant.py`):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VOICE ASSISTANT                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User speaks
    â”‚
    â–¼
ReSpeaker Microphone
    â”‚
    â–¼
Speech Recognition (Google API)
    â”‚
    â”œâ”€> Wake word? ("hey rovy")
    â”‚   â””â”€> Activate assistant
    â”‚
    â””â”€> Question/Command
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                 â”‚                 â”‚
            â–¼                 â–¼                 â–¼
    Movement Command?    Vision Question?   General Question?
            â”‚                 â”‚                 â”‚
            â”‚                 â”‚                 â”‚
    Execute Movement      Capture Image      Text-only
    (via rover_controller)  â”‚                 â”‚
                            â”‚                 â”‚
                            â–¼                 â–¼
                    LLaVA Analysis      LLaVA Analysis
                    (with image)        (text only)
                            â”‚                 â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                            Generate Response
                                     â”‚
                                     â–¼
                            Text-to-Speech
                            (Piper/espeak)
                                     â”‚
                                     â–¼
                            User hears response
```

---

## ğŸ¯ Vision System Deep Dive (Your Area!)

### **`oakd_depth_navigator.py` - How It Works:**

#### **1. OakDDepthCamera Class**
```python
# Initialization
camera = OakDDepthCamera(resolution=(640, 480))
camera.start()

# Captures two streams:
rgb_frame, depth_frame = camera.capture_frames()
# - rgb_frame: Color image (640x480)
# - depth_frame: Depth map in millimeters (640x480)
```

**What happens inside:**
- Creates DepthAI pipeline
- Configures RGB camera (640x480)
- Configures stereo cameras (left + right for depth)
- Stereo matching â†’ depth map
- Outputs both via queues

#### **2. DepthNavigator Class**
```python
navigator = DepthNavigator(safe_distance_mm=800)
command = navigator.get_navigation_command(rgb_frame, depth_frame)
```

**Decision Process:**

```
Step 1: Extract Middle Strip (35%-65% of height)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  (ignored - sky/ceiling)    â”‚ â† Top 35%
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚ â† Middle strip (analyzed)
    â”‚  â•‘ L â”‚ L â”‚ C â”‚ R â”‚ R â•‘  â”‚     (obstacles at robot height)
    â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  (ignored - ground)         â”‚ â† Bottom 35%
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Divide into 5 Regions
    â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
    â”‚ Far  â”‚ Left â”‚Centerâ”‚ Rightâ”‚ Far  â”‚
    â”‚ Left â”‚      â”‚      â”‚      â”‚ Rightâ”‚
    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Step 3: Calculate Clearance Score for Each
    - Filter valid depths (0 < depth < 5000mm)
    - Calculate median distance
    - Score: 0.0 (blocked) to 1.0 (clear)
    - Formula: Linear mapping from 400mm (min) to 2000mm (max)

Step 4: Safety-First Decision
    - Only consider regions with score â‰¥ 0.35 (safety threshold)
    - If forward is safe AND competitive â†’ go forward
    - If forward blocked â†’ choose best side (left/right)
    - If all blocked â†’ random turn to explore

Step 5: Speed Adjustment
    - High clearance (>0.8) â†’ slow speed (0.3m)
    - Medium clearance (0.6-0.8) â†’ slow speed (0.2m)
    - Low clearance (<0.6) â†’ very slow (0.15m)
```

**Output Format:**
```python
{
    'action': 'forward',  # or 'left', 'right', 'stop'
    'speed': 'slow',
    'distance': 0.3,
    'reasoning': 'Forward safe & competitive (C=75% â‰¥ 85% of best=80%)',
    'scores': {
        'far_left': 0.6,
        'left': 0.7,
        'center': 0.75,  # Best path
        'right': 0.65,
        'far_right': 0.5
    }
}
```

---

## ğŸ”Œ How Components Connect

### **Connection Map:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         JETSON NANO - The Powerful Brain ğŸ§                    â”‚
â”‚         (Runs all AI, vision processing, Python code)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
USB Port 1:              USB Port 2:            USB Port 3:
Oak-D Camera             ESP32 Rover             ReSpeaker Mic
    â”‚                    (Serial)                    â”‚
    â”‚                        â”‚                       â”‚
    â””â”€> depthai          pyserial              PyAudio
        library               â”‚                       â”‚
        â”‚                    â”‚                       â”‚
        â–¼                    â–¼                       â–¼
oakd_depth_navigator.py  rover_controller.py  smart_assistant.py
        â”‚                    â”‚                       â”‚
        â”‚                    â””â”€> JSON commands      â”‚
        â”‚                    {"T":1,"L":0.5,"R":0.5} â”‚
        â”‚                                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    USB Port 4: USB Speakers
                              â”‚
                              â””â”€> aplay (TTS output)
```

**Key Point**: All Python code, AI models, and vision processing run on the **Jetson Nano**. The ESP32 is just a motor controller that receives commands.

### **Software Connections:**

```
smart_assistant.py
    â”‚
    â”œâ”€> Uses rover_controller.py (Rover class)
    â”‚   â””â”€> For movement commands
    â”‚
    â”œâ”€> Uses oakd_depth_navigator.py (OakDDepthCamera)
    â”‚   â””â”€> For vision questions ("what do you see?")
    â”‚
    â””â”€> Uses llava_cpp_navigator.py (LLaVACppNavigator)
        â””â”€> For AI understanding

depth_llava_nav.py
    â”‚
    â”œâ”€> Uses rover_controller.py (Rover class)
    â”‚   â””â”€> For motor control
    â”‚
    â”œâ”€> Uses oakd_depth_navigator.py
    â”‚   â”œâ”€> OakDDepthCamera (frame capture)
    â”‚   â””â”€> DepthNavigator (obstacle avoidance)
    â”‚
    â””â”€> Uses llava_cpp_navigator.py (LLaVACppNavigator)
        â””â”€> For strategic guidance
```

---

## ğŸš€ Running the System

### **1. Autonomous Navigation** (Vision + AI):
```bash
python depth_llava_nav.py --duration 300 --safe-distance 800
```
- Runs for 300 seconds
- Combines depth perception + LLaVA AI
- Navigates autonomously

### **2. Voice Assistant**:
```bash
python smart_assistant.py --port /dev/ttyACM0
```
- Listens for wake word ("hey rovy")
- Responds to questions
- Can control movement via voice

### **3. Status Display**:
```bash
python display_status.py --mode simple
```
- Shows battery, temperature, status on OLED

---

## ğŸ¨ Key Design Patterns

### **1. Threading Architecture** (`depth_llava_nav.py`):
- **Producer-Consumer**: Camera thread produces frames â†’ queue â†’ navigation thread consumes
- **Thread Safety**: Uses `threading.Lock()` for shared LLaVA guidance
- **Non-blocking**: LLaVA loads in background, doesn't block startup

### **2. Safety-First Navigation**:
- Depth system has priority (immediate safety)
- LLaVA provides suggestions, but depth must confirm
- Emergency stops if clearance drops suddenly

### **3. Modular Design**:
- Each component is independent
- Can run vision-only, assistant-only, or combined
- Easy to test individual components

---

## ğŸ”§ Recent Changes (What Team Leader Modified)

Based on code analysis, likely changes:

1. **Removed `rotate_and_scan` function** from `DepthNavigator`
   - Comment says: "It conflicted with _capture_thread"
   - Now uses continuous frame capture instead

2. **Frame Queue System** in `depth_llava_nav.py`
   - Single capture thread feeds multiple consumers
   - Prevents conflicts between LLaVA and depth navigation

3. **Thread-Safe LLaVA Guidance**
   - Uses `threading.Lock()` to protect shared state
   - Prevents race conditions

4. **Emergency Stop Logic**
   - Detects sudden clearance drops
   - Prevents collisions

---

## ğŸ“Š Vision System Parameters

### **DepthNavigator Settings:**
- `safe_distance_mm`: 800mm (default) - minimum safe distance
- `warning_distance_mm`: 1200mm (1.5x safe) - early warning
- `blocked_distance_mm`: 600mm (0.75x safe) - too close

### **Region Analysis:**
- Middle strip: 35%-65% of image height (robot-height obstacles)
- 5 regions: far_left, left, center, right, far_right
- Safety threshold: 0.35 (35% clearance minimum)

### **Speed Control:**
- Always "slow" for safety
- Distance: 0.15m - 0.3m based on clearance
- Emergency stop if clearance drops >30%

---

## ğŸ› Debugging Tips

### **Vision Issues:**
1. Check Oak-D camera connection: `lsusb | grep DepthAI`
2. Test camera: Run `oakd_depth_navigator.py` standalone
3. Check depth values: Print median distances in each region

### **Navigation Issues:**
1. Check rover connection: `ls /dev/ttyACM*`
2. Test motors: Use `rover_controller.py` directly
3. Check frame queue: Monitor queue size in `depth_llava_nav.py`

### **AI Issues:**
1. Check LLaVA model paths in `llava_cpp_navigator.py`
2. Monitor GPU memory: `nvidia-smi`
3. Check inference time: Should be <5s per analysis

---

## ğŸ“ Summary

**Your Vision System (`oakd_depth_navigator.py`):**
- Captures RGB + depth from Oak-D camera
- Analyzes depth map for obstacles
- Returns safe navigation commands
- Used by autonomous navigation system

**How It All Fits:**
- Vision provides real-time obstacle avoidance
- AI (LLaVA) provides strategic understanding
- Voice assistant provides user interaction
- Rover controller executes all movement commands

**Key Insight:**
The system uses a **two-layer approach**:
1. **Reactive Layer** (Depth): Fast, safety-first obstacle avoidance
2. **Strategic Layer** (LLaVA): High-level scene understanding and planning

Both work together for intelligent autonomous navigation!

