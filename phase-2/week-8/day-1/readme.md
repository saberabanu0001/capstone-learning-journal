# ğŸ§¾ Vision & Voice System for Capstone Robot
## ğŸ“Œ Project Overview

This module is part of the Capstone Robot Project developed at Sejong University.
It integrates real-time computer vision and voice feedback using an OAK-D Lite AI camera and DepthAI SDK, designed to give the robot spatial awareness and vocal interaction capability.

**Over the past two days, we built and tested:**

- A real-time object detection system (YOLOv8n Spatial Detection Network).

- A macOS-compatible voice alert system that speaks detected objects aloud.

- A modular DepthAI pipeline compatible with SLAM and future motion systems.




## âš™ï¸ System Architecture

**Components:**

- OAK-D Lite Camera â†’ Provides RGB + Depth input.

- DepthAI SDK (v2.30.0) â†’ Builds real-time vision pipeline.

- YOLOv8n Model (COCO dataset) â†’ Detects common objects with bounding boxes and depth data.

- Mac TTS (Text-to-Speech) â†’ Gives audio feedback for detected objects.

- OpenCV â†’ Displays bounding boxes and object labels.



## ğŸ¯ Key Features
## Feature	Description
- ğŸ¥ Object Detection	- Detects people, bottles, cups, plants, and phones in real time.
- ğŸ§  Spatial Awareness	- Measures distance (depth) of objects from the robot camera.
- ğŸ—£ï¸ Voice Announcements	- The robot audibly announces detected objects and their distance (macOS say command).
- ğŸ“¦ Threaded Detection Queue	- Processes detections in parallel for smooth real-time updates.
- âš¡ Modular Code	- Vision and voice modules separated for easy integration with SLAM and motion control.
- ğŸŒ— Lighting Adjustment	- Automatic frame normalization for better low-light detection.



## ğŸ—£ï¸ Voice Functionality (macOS)

### The voice module uses:

***subprocess.Popen(['say', phrase])***


#### Example output:

***â€œPerson detected 1.2 meters away.â€***

#### It triggers when:

***Confidence â‰¥ 70%***

- At least 4 seconds have passed since the last spoken alert for the same label

- This ensures natural feedback without repetition or lag.